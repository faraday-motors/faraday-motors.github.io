{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import *\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read merged data file with charging stations and ACS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30514,) (30514, 6) [  6.40200000e+03   4.97188379e-01   1.94000000e+01   6.00000000e-01\n",
      "   0.00000000e+00   0.00000000e+00] 1.0\n",
      "(30514, 6) (30514,) [  1.21300000e+03   4.50123660e-01   3.97000000e+01   0.00000000e+00\n",
      "   3.52120000e+04   8.70689655e-01] 0.0\n",
      "float64 float64\n",
      "float64\n",
      "30514 22885\n",
      "(22885, 6) (22885,)\n",
      "(7629, 6) (7629,)\n",
      "[ 0.  0.  0.  0.  0.  0.] 0.0\n",
      "[  3.23560000e+04   5.01730745e-01   3.64000000e+01   2.18000000e+01\n",
      "   4.57570000e+04   6.60303392e-01] 2.0\n"
     ]
    }
   ],
   "source": [
    "#import the merged charging station and ACS data\n",
    "# lines = open('data/add_0_stations.csv').read().splitlines()\n",
    "# preprocessed the data file and removed missing values\n",
    "# to do: use imputer to handle missing vaules in scikit\n",
    "# http://stackoverflow.com/questions/30317119/classifiers-in-scikit-learn-that-handle-nan-null?lq=1\n",
    "lines = open('data/missing_values_removed.csv').read().splitlines()\n",
    "#Remove the first row, it's just the headings\n",
    "lines.pop(0)\n",
    "\n",
    "zip_code, count, population, male_pct, age, college_pct, income, home_pct = [], [], [], [], [], [], [], []\n",
    "for line in lines:\n",
    "    items = line.split(',')\n",
    "    zip_code.append(items[0])\n",
    "    count.append(float(items[1]))\n",
    "    population.append(float(items[2]))\n",
    "    male_pct.append(float(items[3]))\n",
    "    age.append(float(items[4]))\n",
    "    college_pct.append(float(items[5]))\n",
    "    income.append(float(items[6]))\n",
    "    home_pct.append(float(items[7]))\n",
    "    \n",
    "# combine lists to numpy arrays for data and labels\n",
    "Y=np.array(count)\n",
    "X=np.vstack((population,male_pct, age, college_pct, income, home_pct))\n",
    "# X=np.vstack((population, male_pct, home_pct))\n",
    "X=np.transpose(X)\n",
    "print Y.shape, X.shape, X[0], Y[0]\n",
    "\n",
    "# add shuffle code before splitting the data into train and dev sets\n",
    "# easiest shuffle utility is sklearn shuffle function\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# shuffle two arrays in unison\n",
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "print X.shape, Y.shape, X[0], Y[0]\n",
    "# determine the data type of the two arrays\n",
    "print X.dtype, Y.dtype\n",
    "# convert to strings\n",
    "# must handle missing values first by making missing values zeros\n",
    "X[X==''] = '0'\n",
    "# now convert to float or leave them alone - converting now to make the code cleaner\n",
    "X1=X.astype(np.float)\n",
    "print X1.dtype\n",
    "\n",
    "# Split the imported data into training 75% and development data 25%\n",
    "\n",
    "split = 3*len(zip_code)/4\n",
    "\n",
    "print len(zip_code), split\n",
    "\n",
    "train_data, train_labels = X[:split], Y[:split]\n",
    "dev_data, dev_labels = X[split:], Y[split:]\n",
    "\n",
    "print train_data.shape, train_labels.shape\n",
    "print dev_data.shape, dev_labels.shape\n",
    "print train_data[0], train_labels[0]\n",
    "print dev_data[0], dev_labels[0]\n",
    "# dev_dat=np.array(dev_data)\n",
    "# print dev_dat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coefficients: \\n', array([  2.06276543e-05,   7.74429448e-01,   7.38819633e-03,\n",
      "         9.43223599e-03,   1.42421493e-05,  -1.71728883e+00]))\n",
      "Variance score: 0.19\n"
     ]
    }
   ],
   "source": [
    "# try lineary regression mode\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(train_data, train_labels)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# score\n",
    "print('Variance score: %.2f' % regr.score(dev_data, dev_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.861580810067\n"
     ]
    }
   ],
   "source": [
    "# fit the first model with logistic regression \n",
    "#\n",
    "logreg = LogisticRegression(penalty='l1', C=1.0, tol=0.01) \n",
    "logreg.fit(train_data, train_labels)\n",
    "print logreg.score(dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
